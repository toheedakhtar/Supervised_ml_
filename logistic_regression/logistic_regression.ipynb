{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function : \n",
    " \n",
    "$g(z) = 1 / 1 + e^{-z}$   \n",
    "\n",
    "Allows us to determine if z is 1/0 or True/False or Yes/No\n",
    "\n",
    "where $z$ = $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e$ is exponential, which numpy has exp() for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to exp: [1 2 3]\n",
      "Output of exp: [ 2.71828183  7.3890561  20.08553692]\n",
      "Input to exp: 1\n",
      "Output of exp: 2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "# Input is an array. \n",
    "input_array = np.array([1,2,3])\n",
    "exp_array = np.exp(input_array)\n",
    "\n",
    "print(\"Input to exp:\", input_array)\n",
    "print(\"Output of exp:\", exp_array)\n",
    "\n",
    "# Input is a single number\n",
    "input_val = 1  \n",
    "exp_val = np.exp(input_val)\n",
    "\n",
    "print(\"Input to exp:\", input_val)\n",
    "print(\"Output of exp:\", exp_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function in python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (z), Output (sigmoid(z))\n",
      "[[-1.000e+01  4.540e-05]\n",
      " [-9.000e+00  1.234e-04]\n",
      " [-8.000e+00  3.354e-04]\n",
      " [-7.000e+00  9.111e-04]\n",
      " [-6.000e+00  2.473e-03]\n",
      " [-5.000e+00  6.693e-03]\n",
      " [-4.000e+00  1.799e-02]\n",
      " [-3.000e+00  4.743e-02]\n",
      " [-2.000e+00  1.192e-01]\n",
      " [-1.000e+00  2.689e-01]\n",
      " [ 0.000e+00  5.000e-01]\n",
      " [ 1.000e+00  7.311e-01]\n",
      " [ 2.000e+00  8.808e-01]\n",
      " [ 3.000e+00  9.526e-01]\n",
      " [ 4.000e+00  9.820e-01]\n",
      " [ 5.000e+00  9.933e-01]\n",
      " [ 6.000e+00  9.975e-01]\n",
      " [ 7.000e+00  9.991e-01]\n",
      " [ 8.000e+00  9.997e-01]\n",
      " [ 9.000e+00  9.999e-01]\n",
      " [ 1.000e+01  1.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Generate an array of evenly spaced values between -10 and 10\n",
    "z_tmp = np.arange(-10,11)\n",
    "\n",
    "# Use the function implemented above to get the sigmoid values\n",
    "y = sigmoid(z_tmp)\n",
    "\n",
    "# Code for pretty printing the two arrays next to each other\n",
    "np.set_printoptions(precision=3) \n",
    "print(\"Input (z), Output (sigmoid(z))\")\n",
    "print(np.c_[z_tmp, y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function for logistic regression  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$  \n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_= 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (array_like Shape (m,)) target value \n",
    "      w : (array_like Shape (n,)) Values of parameters of the model      \n",
    "      b : scalar Values of bias parameter of the model\n",
    "      lambda_: unused placeholder\n",
    "    Returns:\n",
    "      total_cost: (scalar)         cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    total_cost = cost/m\n",
    "\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$  \n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}$$  \n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=None): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) variable such as house size \n",
    "      y : (array_like Shape (m,1)) actual value \n",
    "      w : (array_like Shape (n,1)) values of parameters of the model      \n",
    "      b : (scalar)                 value of parameter of the model \n",
    "      lambda_: unused placeholder.\n",
    "    Returns\n",
    "      dj_dw: (array_like Shape (n,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db: (scalar)                The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          \n",
    "        err_i  = f_wb_i  - y[i]                       \n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      \n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   \n",
    "    dj_db = dj_db/m                                   \n",
    "        \n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (w,b) / parameters using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating logisitc regression (Y/target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = np.dot(X[i],w) \n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "            # Add the corresponding term to z_wb\n",
    "            z_wb += 0\n",
    "        \n",
    "        # Add bias term \n",
    "        z_wb += b\n",
    "        \n",
    "        # Calculate the prediction for this example\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = 1 if f_wb>0.5 else 0\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression \n",
    "###  Cost function for regularized logistic regression\n",
    "\n",
    "regularized logistic regression, the cost function is of the form  \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$  \n",
    "\n",
    "cost function without regularization  \n",
    "$$J(\\mathbf{w}.b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\right]$$  \n",
    "\n",
    "Difference is of regulaization term i.e  \n",
    "$$\\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Calls the compute_cost function that you implemented above\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    \n",
    "    # You need to calculate this value\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    reg_cost = sum(np.square(w))\n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # Add the regularization cost to get the total cost\n",
    "    total_cost = cost_without_reg + (lambda_/(2 * m)) * reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient for regularized logistic regression\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\left( \\frac{1}{m}  \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} \\right) + \\frac{\\lambda}{m} w_j $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    " \n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "       \n",
    "        \n",
    "    return dj_db, dj_dw\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
